# LLM Review Notebook

This notebook provides a comprehensive comparison of the outputs generated by the five biggest language model providers: OpenAI, Google, Anthropic, Meta, and Mistral. The focus of the comparison includes various aspects such as cost, quality, speed, context length, and the distinction between open and closed models. The notebook is structured to showcase the capabilities of these models through different scenarios, including text generation, multimodal understanding, and retrieval-augmented generation (RAG).


## Table of Contents
1. Setup
2. Questions and Answers
3. Generate Model Outputs
4. Testing Multimodal Capabilities
5. Retrieval Augmented Generation (RAG) vs Long Context Length

## Setup

The notebook begins with the installation of necessary libraries. Ensure you have access to the APIs of the respective models:

```bash
!pip install openai langchain langchain-google-genai langchain-anthropic langchain-openai pandas google-generativeai IPython singlestoredb
```

Set up your API keys for OpenAI, Google, and Anthropic:

```python
openai_api_key = "Your Key"
google_api_key = "Your Key"
anthropic_api_key = "Your Key"
```

## Questions and Answers

A set of predefined basic and complex STEM questions is used to evaluate the models. The correct answers are stored for comparison:

```python
questions = [
    "What is the capital of France?",
    "Explain the theory of relativity in two sentences.",
    "What is the derivative of x^2?",
    "Translate 'The quick brown fox jumps over the lazy dog' to French.",
    "Solve the equation: 2x + 3 = 7."
]

correct_answers = [
    "Paris",
    "The theory of relativity, developed by Albert Einstein, states that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is constant regardless of the speed at which an observer travels. This theory revolutionized our understanding of space, time, and gravity.",
    "2x",
    "Le rapide renard brun saute par-dessus le chien paresseux.",
    "x = 2"
]

qa_df = pd.DataFrame({
    'Question': questions,
    'Correct Answer': correct_answers
})
```

## Generate Model Outputs

The notebook generates responses from each model for the predefined questions. Below is an example of how responses are generated for GPT-4:

```python
from langchain_openai import ChatOpenAI

gpt = ChatOpenAI(model="gpt-4o", temperature=0, api_key=openai_api_key)

gpt_responses = []
for question in questions:
    try:
        response = gpt.invoke([("system", "You are a helpful assistant"), ("human", question)])
        gpt_responses.append(response.content)
    except Exception as e:
        gpt_responses.append(f"Error: {str(e)}")

qa_df['GPT-4 Response'] = gpt_responses
qa_df.to_csv('model_responses.csv', index=False)
print("Responses saved to model_responses.csv")
```

## Testing Multimodal Capabilities

The notebook also tests the models' multimodal capabilities by analyzing an image and generating descriptive content:

```python
from openai import OpenAI

client = OpenAI(api_key=openai_api_key)

response = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Whatâ€™s in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0].message.content)
```

## Retrieval Augmented Generation (RAG) vs Long Context Length

### Long Context Length Test

Tests how models handle long context inputs:

```python
from langchain_google_genai import ChatGoogleGenerativeAI

gemini = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0, google_api_key=google_api_key)

with open('superbowl.txt', 'r') as file:
    text_content = file.read()

output = gemini.invoke([
    ("system", "Answer based on the users content (3 sentences max): Which celebrities were at the 2024 superbowl?"),
    ("human", text_content),
])

Markdown(output.content)
```

### RAG Test

Demonstrates the use of retrieval-augmented generation to enhance responses based on document search:

```python
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores.singlestoredb import SingleStoreDB
from openai import OpenAI
import os
import google.generativeai as gemini

os.environ["SINGLESTOREDB_URL"] = "INSERT DB URL"

# Load and process documents
loader = TextLoader("superbowl.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# Generate embeddings and create a document search database
embeddings = OpenAIEmbeddings(api_key=openai_api_key)
docsearch = SingleStoreDB.from_documents(docs, embeddings, table_name="superbowl")

gemini.configure(api_key=google_api_key)

# Chat loop
while True:
    user_query = input("\nYou: ")

    if user_query.lower() in ['quit', 'exit']:
        print("Exiting chatbot.")
        break

    docs = docsearch.similarity_search(user_query)
    if docs:
        context = docs[0].page_content

        model = gemini.GenerativeModel('gemini-pro')

        response = model.generate_content(user_query + context)

        print("AI: ", end="")
        for chunk in response:
            print(chunk.text)
            print("_" * 80)

    else:
        print("AI: Sorry, I couldn't find relevant information.")
```

This notebook offers a detailed comparison and practical evaluation of various leading language models, highlighting their strengths and weaknesses in different contexts. The generated outputs are stored in a CSV file for further analysis and comparison.
